2022-11-09 14:33:06:INFO: Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2022-11-09 14:33:06:INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, config_name='data/berts/bert/config.json', data_dir='data/dataset/NER', default_label='O', device=device(type='cpu'), do_eval=False, do_predict=False, do_shuffle=True, do_train=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_file='data/dataset/NER/label.txt', learning_rate=0.0001, local_rank=-1, logging_dir='data/log', logging_steps=4, max_grad_norm=1.0, max_scan_num=10000, max_seq_length=48, max_steps=-1, max_word_num=5, model_name_or_path=None, model_type='Bert_Token', n_gpu=0, no_cuda=False, nodes=1, num_train_epochs=2, output_dir='data/result', overwrite_cache=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=800, save_total_limit=50, saved_embedding_dir='data/embedding', seed=106524, sgd_momentum=0.9, vocab_file='data/berts/bert/vocab.txt', warmup_steps=95, weight_decay=0.0, word_embed_dim=200, word_embedding='data/embedding/word_embedding.txt', word_vocab_file='data/vocab/final_vocab.txt')
2022-11-09 14:34:05:INFO: Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2022-11-09 14:34:05:INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, config_name='data/berts/bert/config.json', data_dir='data/dataset/NER', default_label='O', device=device(type='cpu'), do_eval=False, do_predict=False, do_shuffle=True, do_train=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_file='data/dataset/NER/label.txt', learning_rate=0.0001, local_rank=-1, logging_dir='data/log', logging_steps=4, max_grad_norm=1.0, max_scan_num=10000, max_seq_length=48, max_steps=-1, max_word_num=5, model_name_or_path=None, model_type='Bert_Token', n_gpu=0, no_cuda=False, nodes=1, num_train_epochs=2, output_dir='data/result', overwrite_cache=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=800, save_total_limit=50, saved_embedding_dir='data/embedding', seed=106524, sgd_momentum=0.9, vocab_file='data/berts/bert/vocab.txt', warmup_steps=95, weight_decay=0.0, word_embed_dim=200, word_embedding='data/embedding/word_embedding.txt', word_vocab_file='data/vocab/final_vocab.txt')
2022-11-09 14:36:46:INFO: Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2022-11-09 14:36:46:INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, config_name='data/berts/bert/config.json', data_dir='data/dataset/NER', default_label='O', device=device(type='cpu'), do_eval=False, do_predict=False, do_shuffle=True, do_train=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_file='data/dataset/NER/label.txt', learning_rate=0.0001, local_rank=-1, logging_dir='data/log', logging_steps=4, max_grad_norm=1.0, max_scan_num=10000, max_seq_length=48, max_steps=-1, max_word_num=5, model_name_or_path=None, model_type='Bert_Token', n_gpu=0, no_cuda=False, nodes=1, num_train_epochs=2, output_dir='data/result', overwrite_cache=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=800, save_total_limit=50, saved_embedding_dir='data/embedding', seed=106524, sgd_momentum=0.9, vocab_file='data/berts/bert/vocab.txt', warmup_steps=95, weight_decay=0.0, word_embed_dim=200, word_embedding='data/embedding/word_embedding.txt', word_vocab_file='data/vocab/final_vocab.txt')
2022-11-09 14:36:59:INFO: Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2022-11-09 14:36:59:INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, config_name='data/berts/bert/config.json', data_dir='data/dataset/NER', default_label='O', device=device(type='cpu'), do_eval=False, do_predict=False, do_shuffle=True, do_train=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_file='data/dataset/NER/label.txt', learning_rate=0.0001, local_rank=-1, logging_dir='data/log', logging_steps=4, max_grad_norm=1.0, max_scan_num=10000, max_seq_length=48, max_steps=-1, max_word_num=5, model_name_or_path=None, model_type='Bert_Token', n_gpu=0, no_cuda=False, nodes=1, num_train_epochs=2, output_dir='data/result', overwrite_cache=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=800, save_total_limit=50, saved_embedding_dir='data/embedding', seed=106524, sgd_momentum=0.9, vocab_file='data/berts/bert/vocab.txt', warmup_steps=95, weight_decay=0.0, word_embed_dim=200, word_embedding='data/embedding/word_embedding.txt', word_vocab_file='data/vocab/final_vocab.txt')
2022-11-09 14:37:17:INFO: Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2022-11-09 14:37:17:INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, config_name='data/berts/bert/config.json', data_dir='data/dataset/NER', default_label='O', device=device(type='cpu'), do_eval=False, do_predict=False, do_shuffle=True, do_train=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_file='data/dataset/NER/label.txt', learning_rate=0.0001, local_rank=-1, logging_dir='data/log', logging_steps=4, max_grad_norm=1.0, max_scan_num=10000, max_seq_length=48, max_steps=-1, max_word_num=5, model_name_or_path=None, model_type='Bert_Token', n_gpu=0, no_cuda=False, nodes=1, num_train_epochs=2, output_dir='data/result', overwrite_cache=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=800, save_total_limit=50, saved_embedding_dir='data/embedding', seed=106524, sgd_momentum=0.9, vocab_file='data/berts/bert/vocab.txt', warmup_steps=95, weight_decay=0.0, word_embed_dim=200, word_embedding='data/embedding/word_embedding.txt', word_vocab_file='data/vocab/final_vocab.txt')
2022-11-20 21:16:50:INFO: Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2022-11-20 21:16:50:INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, config_name='data/berts/bert/config.json', data_dir='data/dataset/NER', default_label='O', device=device(type='cpu'), do_eval=False, do_predict=False, do_shuffle=True, do_train=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_file='data/dataset/NER/label.txt', learning_rate=0.0001, local_rank=-1, logging_dir='data/log', logging_steps=4, max_grad_norm=1.0, max_scan_num=10000, max_seq_length=48, max_steps=-1, max_word_num=5, model_name_or_path=None, model_type='Bert_Token', n_gpu=0, no_cuda=False, nodes=1, num_train_epochs=2, output_dir='data/result', overwrite_cache=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=800, save_total_limit=50, saved_embedding_dir='data/embedding', seed=106524, sgd_momentum=0.9, vocab_file='data/berts/bert/vocab.txt', warmup_steps=95, weight_decay=0.0, word_embed_dim=200, word_embedding='data/embedding/word_embedding.txt', word_vocab_file='data/vocab/final_vocab.txt')
2022-11-28 15:38:19:INFO: Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2022-11-28 15:38:19:INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, config_name='data/berts/bert/config.json', data_dir='data/dataset/NER', default_label='O', device=device(type='cpu'), do_eval=False, do_predict=False, do_shuffle=True, do_train=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_file='data/dataset/NER/label.txt', learning_rate=0.0001, local_rank=-1, logging_dir='data/log', logging_steps=4, max_grad_norm=1.0, max_scan_num=10000, max_seq_length=48, max_steps=-1, max_word_num=5, model_name_or_path=None, model_type='Bert_Token', n_gpu=0, no_cuda=False, nodes=1, num_train_epochs=2, output_dir='data/result', overwrite_cache=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=800, save_total_limit=50, saved_embedding_dir='data/embedding', seed=106524, sgd_momentum=0.9, vocab_file='data/berts/bert/vocab.txt', warmup_steps=95, weight_decay=0.0, word_embed_dim=200, word_embedding='data/embedding/word_embedding.txt', word_vocab_file='data/vocab/final_vocab.txt')
2022-11-28 15:39:07:INFO: Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2022-11-28 15:39:07:INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, config_name='data/berts/bert/config.json', data_dir='data/dataset/NER', default_label='O', device=device(type='cpu'), do_eval=False, do_predict=False, do_shuffle=True, do_train=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_file='data/dataset/NER/label.txt', learning_rate=0.0001, local_rank=-1, logging_dir='data/log', logging_steps=4, max_grad_norm=1.0, max_scan_num=10000, max_seq_length=48, max_steps=-1, max_word_num=5, model_name_or_path=None, model_type='Bert_Token', n_gpu=0, no_cuda=False, nodes=1, num_train_epochs=2, output_dir='data/result', overwrite_cache=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=800, save_total_limit=50, saved_embedding_dir='data/embedding', seed=106524, sgd_momentum=0.9, vocab_file='data/berts/bert/vocab.txt', warmup_steps=95, weight_decay=0.0, word_embed_dim=200, word_embedding='data/embedding/word_embedding.txt', word_vocab_file='data/vocab/final_vocab.txt')
2022-11-28 16:56:27:INFO: Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2022-11-28 16:56:27:INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, config_name='data/berts/bert/config.json', data_dir='data/dataset/NER', default_label='O', device=device(type='cpu'), do_eval=False, do_predict=False, do_shuffle=True, do_train=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_file='data/dataset/NER/label.txt', learning_rate=0.0001, local_rank=-1, logging_dir='data/log', logging_steps=4, max_grad_norm=1.0, max_scan_num=10000, max_seq_length=48, max_steps=-1, max_word_num=5, model_name_or_path=None, model_type='Bert_Token', n_gpu=0, no_cuda=False, nodes=1, num_train_epochs=2, output_dir='data/result', overwrite_cache=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=800, save_total_limit=50, saved_embedding_dir='data/embedding', seed=106524, sgd_momentum=0.9, vocab_file='data/berts/bert/vocab.txt', warmup_steps=95, weight_decay=0.0, word_embed_dim=200, word_embedding='data/embedding/word_embedding.txt', word_vocab_file='data/vocab/final_vocab.txt')
2022-11-28 16:57:47:INFO: Process rank: -1, device: cpu, n_gpu: 0, distributed training: False, 16-bits training: False
2022-11-28 16:57:47:INFO: Training/evaluation parameters Namespace(adam_epsilon=1e-08, config_name='data/berts/bert/config.json', data_dir='data/dataset/NER', default_label='O', device=device(type='cpu'), do_eval=False, do_predict=False, do_shuffle=True, do_train=False, evaluate_during_training=False, fp16=False, fp16_opt_level='O1', gradient_accumulation_steps=1, label_file='data/dataset/NER/label.txt', learning_rate=0.0001, local_rank=-1, logging_dir='data/log', logging_steps=4, max_grad_norm=1.0, max_scan_num=10000, max_seq_length=48, max_steps=-1, max_word_num=5, model_name_or_path=None, model_type='Bert_Token', n_gpu=0, no_cuda=False, nodes=1, num_train_epochs=2, output_dir='data/result', overwrite_cache=True, per_gpu_eval_batch_size=4, per_gpu_train_batch_size=4, save_steps=800, save_total_limit=50, saved_embedding_dir='data/embedding', seed=106524, sgd_momentum=0.9, vocab_file='data/berts/bert/vocab.txt', warmup_steps=95, weight_decay=0.0, word_embed_dim=200, word_embedding='data/embedding/word_embedding.txt', word_vocab_file='data/vocab/final_vocab.txt')
